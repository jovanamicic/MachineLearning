Lasso nam omogučava selekciju obeležja.
Za optimizaciju lasso funkcije greške ne možemo koristiti
standardandan gradient descent (jer funkcija apsolutne vrednosti
nije diferencijabilna u svim tačkama) → naučili smo coordinate descent algoritam.

                                def main():
    # prepare data
    trainingSet, testSet, trainSetGrades, testSetGrades, trainSetFeatures, testSetFeatures = load_dataset2('train.csv', 'test.csv')
    print 'Train set: ' + repr(len(trainingSet))
    print 'Test set: ' + repr(len(testSet))
    print
    
    y_cd = trainSetGrades
    x_cd = trainingSet
    
    model = Lasso(alpha=2.0, max_iter=1000)
    model.fit(x_cd, y_cd)
    print '***'
    br = 0
    for c in model.coef_:
        x = round(c)
        if x == 0:
            br += 1
            
    print br
    
    
#    meanTrainingSet = np.mean(trainingSet)
#    stdTrainingSet = np.std(trainingSet)
#    
#    meanTestSet = np.mean(testSet)
#    stdTestSet = np.std(testSet)
#    
#    meanTestGradeSet = np.mean(testSetGrades)
#    stdTestGradeSet = np.std(testSetGrades)
#    
#    trainingSetNor = (trainingSet - meanTrainingSet) / stdTrainingSet
#    testSetNor = (testSet - meanTestSet) / stdTestSet

    # generate predictions
    predictions=[]
    k = 7
    for x in range(len(testSet)):
        neighbors = get_neighbors(trainingSet, testSet[x], k)
        result = get_response(neighbors)
        predictions.append(result)
        #print('> predicted=' + repr(result) + ', actual=' + repr(testSet[x][-1]))

#    for x in range(len(testSet)):
#        neighbors = get_neighbors(trainingSetNor, testSetNor[x], k)
#        result = get_response(neighbors)
#        predictions.append(result)
#        print('> predicted=' + repr(result) + ', actual=' + repr(testSetNor[x][-1]))
   
    
    rmse = get_rmse(testSetGrades, predictions)
    print 'RMSE: ', rmse
    
    accuracy = get_accuracy(testSet, predictions)
    print('Accuracy: ' + repr(accuracy) + '%')
    
    #main()
